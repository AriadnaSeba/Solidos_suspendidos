---
title: "Análisis de Sólidos Suspendidos"
format: 
  html:
    number-sections: true
    toc: true 
    toc-location: right
    toc-depth: 3
    embed-resources: true
    crossrefs-hover: false
    lang: es
    bibliography: bibliografia/bibliografia.bib
    csl: bibliografia/ieee.csl
date: last-modified
author:
  - name: Víctor Gauto
    orcid: 0000-0001-9960-8558
    corresponding: true
    email: victor.gauto@ca.frre.utn.edu.ar
    affiliations:
      - name: GISTAQ (UTN-FRRe)
        url: https://www.instagram.com/gistaq.utn/
abstract: |
  Este sitio web contiene cuestiones etc
keywords:
  - GISTAQ
  - UTN
  - FRRe
  - Quarto
jupyter: python3
execute:
  echo: true
---

## Sólidos suspendidos totales<span style="font-weight:normal; font-size: 1rem">, por Vera Geneyer (https://github.com/VeraGeneyer)</span> {toc-text="Sólidos suspendidos totales"}

Los sólidos suspendidos totales (TSM): es la cantidad de materia en suspensión en el agua, que incluye plancton, minerales, arena, y microorganismos. Se determinan como el residuo no filtrable de una muestra de agua. Niveles altos (TSM) pueden reducir la transparencia del agua, limitar la luz y y transportar sustancias tóxicas, afectando la vida acuática y la calidad del agua.
Este parámetro, medido mediante sensores remotos, nos da información sobre el estado físico del cuerpo de agua y están relacionados con factores como la humedad, temperatura y entre otros, que es vital para detectar riesgos al ecosistema y cumplir con las normas ambientales.

### Métodos tradicionales

:::: {.content-visible when-format="html"}

::: {.column-screen-right}

| Ecuación | Bandas (nm) | Métricas | Aguas | Plataforma | Referencia |
|:-:|:--|:--|:--|:--|:-:|
| $-229.34 \left( \frac{B03}{B08} \right)^{3}+1001.65 \left( \frac{B03}{B08} \right)^{2}-1422.7 \left( \frac{B03}{B08} \right)+665.17$ | B03, B08 | $R^{2}$ | Embalse^[Aguas lénticas.] | Landsat-8 | @Ramirez2017 |
| $-244.83+40.21 \cdot B01-3.67 \cdot NDWI$ | B01, NDWI (B03, B08) | $R^{2}$, RMSE, d | Río^[d = prueba estadística de <b>Durbin-Watson</b>.] | GeoEye | @Gomez2014 |

: Características principales de algoritmos tradicionales para la estimación de sólidos suspendidos. {#tbl-solsus-trad .striped .hover tbl-colwidths="[50,10,10,10,10]"}

:::

::::

:::: {.content-visible when-format="typst"}

| Ecuación | Referencia |
|:--|:-:|
| $-229.34 \left( \frac{B03}{B08} \right)^{3}+1001.65 \left( \frac{B03}{B08} \right)^{2}-1422.7 \left( \frac{B03}{B08} \right)+665.17$ | [@Ramirez2017] |
| $-244.83+40.21 \cdot B01-3.67 \cdot NDWI$ | [@Gomez2014] |

: Características principales de algoritmos tradicionales para la estimación de sólidos suspendidos. {#tbl-solsus-trad .striped .hover tbl-colwidths="[80,20]"}

::: {.block stroke='rgb("#B86092")' inset="8pt" radius="4pt"}

[Ver tabla completa en la versión online &#x2197;](https://vhgauto.quarto.pub/gistaq-parana/#tbl-solsus-trad)

:::

::::

De acuerdo a un estudio que analizó 48 cuerpos de agua, la estimación de TSM se hizo en su mayoría por modelos lineales, siendo la banda B8A la más frecuente [@Cruz2023].

### Métodos de aprendizaje automático

El **aprendizaje automático (ML)**  es una rama de la inteligencia artificial cuyo objetivo es desarrollar algoritmos capaces de resolver problemas mediante el análisis de datos y la creación de funciones que describen el comportamiento de fenómenos monitoreados [@Carpio2021]. Los modelos de aprendizaje automático más utilizados y mencionados por los investigadores para predecir la concentración de SST son:

* **Bosque Aleatorio (RF) y Refuerzo Adaptativo (AdB)**, modelos que se destacan por su robustez ante datos complejos y ruidosos. Estos algoritmos construyen múltiples árboles de decisión que analizan las relaciones entre características como el uso del suelo o el volumen de escorrentía y los niveles de SST [@Moeini2021].

* **Redes Neuronales Artificiales (ANN)**, copian las redes neuronales biológicas y aprenden patrones complejos en grandes volúmenes de datos, como los niveles de SST en distintas condiciones ambientales [@Moeini2021],

* **k-Nearest Neighbors (kNN)**, en sus variantes de ponderación uniforme y variable, que estima el SST en función de la cercanía en características de nuevos puntos de muestreo con datos históricos [@Moeini2021].

El aprendizaje automático es esencial para mejorar la precisión y rapidez en el análisis de la calidad del agua, proporcionando un monitoreo más eficiente y menos costoso en comparación con los métodos tradicionales, especialmente en áreas de difícil acceso o con datos limitados.

:::: {.content-visible when-format="html"}

::: {.column-screen-right}

| **Modelo de machine learning** | **Software** | **Agua** | **Datos** | **Métricas** | **Referencias** |
|:--|:--|:--|:--|:--|:-:|
|Bagging y Random Forest|Programa R|Bahía|Muestreo|Prueba de normalidad multivalente Mardia-tests y Royston|@Carpio2021|
|Regresión lineal, LASSO, regresión de vectores de soporte (SVR), K vecinos más cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).|-|Lago y embalse|Sentinel-2 y UAV|$R^{2}$| @Silveira2020|
|Regresión lineal, regresión de vectores de soporte (SVR), K vecinos más cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).|Programa Python|Lagos|Estación de monitoreo (Sensores para cada parámetro)|$R^{2}$, NSE y RMSE| @Moeini2021|

: Características principales de algoritmos de aprendizaje automático para la estimación de sólidos suspendidos. {#tbl-solsus-machine .striped .hover tbl-colwidths="[40,12,12,13,13,10]"}

:::

::::

:::: {.content-visible when-format="typst"}

| Modelo de machine learning | Referencias |
|:--|:-:|
|Bagging y Random Forest| [@Carpio2021] |
|Regresión lineal, LASSO, regresión de vectores de soporte (SVR), K vecinos más cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).| [@Silveira2020] |
|Regresión lineal, regresión de vectores de soporte (SVR), K vecinos más cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).| [@Moeini2021] |

: Características principales de algoritmos de aprendizaje automático para la estimación de sólidos suspendidos. {#tbl-solsus-machine .striped .hover tbl-colwidths="[80,20]"}

::: {.block stroke='rgb("#B86092")' inset="8pt" radius="4pt"}

[Ver tabla completa en la versión online &#x2197;](https://vhgauto.quarto.pub/gistaq-parana/#tbl-solsus-machine)

:::

::::


Diferentes metodologías para la resolución de una regresión simple por el método de mínimos cuadrados.

## **Python**, con `sklearn`

Se muestra a continuación el tutorial mostrado en para el modelo [Mínimos cuadrados ordinarios](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#ordinary-least-squares-example).

Importo la librería `sklearn`, funciones de interés y para generar figuras.

```{python}
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
```

Cargo los datos de interés y divido en entrenamiento y validación.

```{python}
X, y = load_diabetes(return_X_y=True)
X = X[:, [2]]  # Use only one feature
X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=20, shuffle=False
)
```

Creo un modelo de regresión y ajusto utilizando los datos de entrenamiento.

```{python}
regressor = LinearRegression().fit(X_train, y_train)
```

Evalúo el modelo generado a partir de las **métricas de desempeño**.

```{python}
y_pred = regressor.predict(X_test)
p_rmse = mean_squared_error(y_test, y_pred)
p_r2 = r2_score(y_test, y_pred)
```

::: {.callout-note icon="false" title="Métricas de desempeño"}
El error cuadrático medio es: `{r} round(py$p_rmse, 3)`.

El coeficiente de determinación (R<sup>2</sup>) es: `{r} round(py$p_r2, 3)`.

:::

Visualizo los resultados comparando el conjunto de entrenamiento y validación.

::: {.column-body-outset}

```{python}
fig, ax = plt.subplots(ncols=2, figsize=(10, 5), sharex=True, sharey=True)

ax[0].plot(
    X_train,
    regressor.predict(X_train),
    linewidth=3,
    color="#17A77E",
    label="Modelo",
)
ax[0].scatter(X_train, y_train, label="Entrenamiento", color = "#9D50A6", alpha = .6)
ax[0].set(xlabel="Característica", ylabel="Objetivo", title="Conjunto de entrenamiento")
ax[0].legend()

ax[1].plot(X_test, y_pred, linewidth=3, color="#17A77E", label="Modelo")
ax[1].scatter(X_test, y_test, label="Validación", color = "#9D50A6", alpha = .6)
ax[1].set(xlabel="Característica", ylabel="Objetivo", title="Conjunto de validación")
ax[1].legend()

fig.suptitle("Regresión lineal")

plt.show()
```

:::


### Procesamiento de datos con Python

Se detalla el procedimiento técnico que implementé para procesar información ambiental georreferenciada con el objetivo de analizar el comportamiento del parámetro **sólidos suspendidos (sol_sus)** en una región específica (pixel `3x3`). Para esto, utilicé el lenguaje Python y la biblioteca `pandas`, que resulta particularmente eficiente para el manejo de estructuras tabulares. 

#### Carga de datos

Primero importo la biblioteca `pandas`, una herramienta en Python que se utiliza para manejar datos en formato tabular (como hojas de cálculo o CSVs). Se le da el alias `pd` por convención, para simplificar el código.

Luego cargo dos archivos CSV con la función `pd.read_csv()`, la cual convierte dichos archivos en objetos del tipo `DataFrame`, que representan tablas en memoria, que son estructuras de datos similares a tablas (parecida a una hoja de Excel). Los conjuntos de datos cargados fueron:

- `gis_df`: contiene información geográfica (latitud, longitud, pixel, etc.).
- `lab_df`: contiene datos de laboratorio, incluyendo el parámetro de interés `sol_sus`.

 Verifico la carga correcta mostrando las primeras filas con la función `.head()`. Es útil para ver rápidamente cómo es la estructura del archivo: qué columnas hay, qué tipo de datos, si se cargó bien.

```{python}
import pandas as pd  # pandas es la biblioteca para manejar datos tabulares

# Cargar los archivos de datos
gis_df = pd.read_csv('datos/base_de_datos_gis.csv')
lab_df = pd.read_csv('datos/base_de_datos_lab.csv')

# Ver las primeras filas para asegurarse de que se cargaron bien
gis_df.head(), lab_df.head()

print("Primeras filas de gis_df:")
display(gis_df.head())

print("\nPrimeras filas de lab_df:")
display(lab_df.head())
```

::: {.dropdown}
<details>
<summary>📄 Nota técnica</summary>

- `pd.read_csv()` carga los archivos en estructuras llamadas *dataframes*, que funcionan como tablas.  
- `head()` te muestra las primeras 5 filas para ver cómo están los datos.
- `display()` permite mostrar las tablas con formato más visual (en HTML).

</details>

:::

#### Filtrar el parámetro 'sol_sus'

En el conjunto de datos del laboratorio `lab_df`, hay múltiples parámetros medidos (como pH, turbidez, etc.). En este caso, me interesa trabajar solamente con los datos de **sólidos suspendidos**, identificado como `"sol_sus"` en la columna `param`. Este filtrado selectivo lo realicé para limitar el análisis al fenómeno físico-químico de interés.

Filtré el DataFrame para quedarme solo con esas filas, y renombré la columna `valor` como `sol_sus` para que sea más claro en los siguientes pasos.

```{python}
# Filtramos solo las filas donde el parámetro es 'sol_sus'
sol_sus_df = lab_df[lab_df["param"] == "sol_sus"]

# Renombramos la columna 'valor' a 'sol_sus' para que tenga sentido en el merge
sol_sus_df = sol_sus_df.rename(columns={"valor": "sol_sus"})

# Mostramos para confirmar
sol_sus_df.head()

print("Primeras filas de sol_sus_df:")
display(sol_sus_df.head())
```

::: {.dropdown}
<details>
<summary>📄 Nota técnica</summary>

- `lab_df[lab_df["param"] == "sol_sus"]` filtra las filas cuyo valor en la columna `"param"` sea `"sol_sus"`.  
- `.rename(columns={"valor": "sol_sus"})` cambia el nombre de la columna `"valor"` a `"sol_sus"`.

</details>

:::

#### Combinar datos geoespaciales y de laboratorio

Ahora quiero unir los datos de ubicación (latitud, longitud) con los valores de sol_sus. Para eso uní ambos conjuntos de datos usando `pd.merge()`, que combina tablas usando columnas comunes.
En este caso, usé las columnas `latitud` y `longitud` para juntar las filas que corresponden a la misma ubicación.
La unión fue de tipo `"inner"`, que mantiene solo las filas que aparecen en ambas tablas, asegurando que solo trabajemos con datos coincidentes.

```{python}
# Unimos por latitud y longitud
df_combinado = pd.merge(
    gis_df,
    sol_sus_df[["latitud", "longitud", "sol_sus"]],
    on=["latitud", "longitud"],
    how="inner"
)

df_combinado.head()
print("Primeras filas de df_combinado:")
display(df_combinado.head())
```

::: {.dropdown}
<details>
<summary>📄 Nota técnica</summary>

- `pd.merge()` permite combinar dos DataFrames en uno nuevo, uniendo filas que coincidan en las columnas especificadas.  
- `on=["latitud", "longitud"]` indica que la combinación debe hacerse usando esas columnas como claves.  
- `how="inner"` especifica el tipo de combinación:  
  - `"inner"`: solo conserva las filas donde hay coincidencia en ambos DataFrames.  
  - Otras opciones:  
    - `"left"`: conserva todas las filas del primer DataFrame.  
    - `"right"`: conserva todas las filas del segundo.  
    - `"outer"`: conserva todo, incluso si no hay coincidencia.

</details>

:::

#### Filtrado espacial por pixel

Luego de combinar los datos, aplico un filtrado adicional al DataFrame sobre la columna `pixel` para conservar únicamente las filas correspondientes al área geográfica designada como `"3x3"`. Este paso reduce el dominio de análisis y permite concentrarse en una región de estudio concreta.

```{python}
# Filtramos solo los datos del pixel 3x3
df_pixel_3x3 = df_combinado[df_combinado["pixel"] == "3x3"]

df_pixel_3x3.head()
print("Primeras filas de df_pixel_3x3:")
display(df_pixel_3x3.head())
```

::: {.dropdown}
<details>
<summary>📄 Nota técnica</summary>

- Usa filtrado booleano (`DataFrame[condición]`), que es la forma estándar en pandas para seleccionar subconjuntos de datos. 
- `df_pixel_3x3 = df_combinado[df_combinado["pixel"] == "3x3"]` selecciona ese subconjunto. Filtra las filas cuyo valor en la columna `"pixel"` es igual a `"3x3"`.

</details>

:::

#### Reordenamiento de columnas

Para facilitar la lectura y el análisis del `DataFrame` final, reorganicé las columnas de manera que las variables `fecha`, `longitud` y `latitud` figuren al comienzo. 

```{python}
# Reordenar las columnas
columnas_ordenadas = ['fecha', 'longitud', 'latitud'] + [col for col in df_pixel_3x3.columns if col not in ['fecha', 'longitud', 'latitud']]
df_reordenado = df_pixel_3x3[columnas_ordenadas]

# Mostramos las primeras filas para verificar
df_reordenado.head()
print("Primeras filas de df_reordenado:")
display(df_reordenado.head())
```

::: {.dropdown}
<details>
<summary>📄 Nota técnica</summary>

- `[col for col in df_pixel_3x3.columns if col not in [...]]` genera una lista con todas las columnas excepto las que se indican explícitamente. Esto permite personalizar el orden de las columnas de forma flexible.   
- `['fecha', 'longitud', 'latitud'] + [...]` reordena las columnas.  
- `df_pixel_3x3[columnas_ordenadas]` reordena efectivamente las columnas del DataFrame con base en la lista definida.

</details>

:::

#### Guardar el archivo final

Finalmente, guardo el resultado como un nuevo archivo .csv dentro de la carpeta datos. 

Por último, exporto el resultado a un nuevo archivo en formato `.csv`, mediante la función `to_csv()` de pandas, con el parámetro `index=False` para evitar que la columna de índice se incluya en el archivo de salida que pandas crea por defecto.
Esto me permite utilizarlo después para visualización o análisis posterior.

```{python}
# Guardar el archivo CSV dentro de la carpeta "datos"
df_reordenado.to_csv('datos/datos_sol_sus_pixel_3x3.csv', index=False)
```

::: {.dropdown}
<details>
<summary>📄 Nota técnica</summary>

- `to_csv()`  guarda los datos en formato CSV.  
- `index=False` evita que se guarde el índice numérico del DataFrame como una columna adicional en el CSV.

</details>

:::


### Análisis de Regresión Lineal

En este análisis aplico un modelo de regresión lineal simple para estudiar la relación entre la **reflectancia** y los **sólidos suspendidos**, utilizando datos experimentales. La regresión lineal es una técnica fundamental del aprendizaje automático supervisado que nos permite predecir un valor continuo basado en una o más variables independientes. A lo largo de este documento, se explican paso a paso las acciones realizadas y los conceptos clave para comprender y replicar este análisis.

#### Importar librerías

En este paso, cargo las bibliotecas necesarias para procesar datos, ajustar modelos de regresión, evaluar su desempeño y visualizar los resultados. 

```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
```

::: {.dropdown}
<details>
<summary>📄 Nota técnica</summary>

> `pandas` se utiliza para manejar datos en forma de tablas (DataFrames), especialmente útiles al trabajar con archivos `.csv`.

> `train_test_split` permite dividir los datos en subconjuntos de entrenamiento y prueba, lo cual es esencial para evaluar el desempeño de un modelo sin sobreajustarlo.

> `LinearRegression` representa un modelo lineal que se ajusta a los datos minimizando el error cuadrático entre las predicciones y los valores reales.

> `mean_squared_error` y `r2_score` son métricas de evaluación: el primero mide el promedio de los errores al cuadrado, mientras que el segundo indica qué tan bien el modelo explica la variabilidad de los datos.

> `matplotlib.pyplot` se utiliza para crear gráficos. Permite visualizar los datos y los resultados del modelo.
 
</details>

:::

#### Cargar datos desde un CSV

Importo el archivo `.csv` con los datos experimentales. Se visualizan las primeras filas para verificar que los datos se han cargado correctamente.

```{python}
# Cargamos el CSV
datos = pd.read_csv('datos/datos_sol_sus_pixel_3x3.csv')

# Mostramos las primeras filas para verificar
datos.head()
print("Primeras filas de datos:")
display(datos.head())
```

::: {.dropdown}
<details>
<summary>📄 Nota técnica</summary>

> `pd.read_csv` carga datos desde un archivo `.csv` y los convierte en un DataFrame de Pandas. Esta estructura tabular permite filtrar, seleccionar y transformar fácilmente los datos.

> `datos.head()` permite ver las primeras 5 filas del DataFrame para tener una vista preliminar de los datos cargados.

</details>

:::

#### Seleccionar variables y dividir en conjuntos

Selecciono las variables relevantes: `reflect` como variable independiente y `sol_sus` como variable dependiente. Luego divido el conjunto en dos subconjuntos: uno para entrenar el modelo y otro para probarlo, lo cual sirve para evaluar su capacidad de generalización.

```{python}
# Selección de variables
X = datos[["reflect"]]
y = datos["sol_sus"]

# División en entrenamiento y validación
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)
```

::: {.dropdown}
<details>
<summary>📄 Nota técnica</summary>

> Se selecciona una columna como variable independiente (X) y otra como variable dependiente (y). Es importante usar doble corchete al seleccionar una sola columna como X para mantener la estructura de tabla.

> `train_test_split` divide el conjunto de datos en entrenamiento y prueba. Esto permite entrenar el modelo en un subconjunto y evaluar su capacidad de generalización con otro.

> El parámetro `test_size=0.2` indica que el 20% de los datos se usan para prueba. `shuffle=False` mantiene el orden original de los datos, útil cuando los datos están organizados temporalmente o espacialmente.

</details>

:::

#### Entrenar modelo de regresión lineal

En este paso se entrena un modelo de regresión lineal usando los datos de entrenamiento. El modelo aprende la relación matemática entre la reflectancia y los sólidos suspendidos.

```{python}
regressor = LinearRegression().fit(X_train, y_train)
```

::: {.dropdown}
<details>
<summary>📄 Nota técnica</summary>

> `LinearRegression().fit()` ajusta un modelo lineal a los datos de entrenamiento. Internamente calcula la pendiente e intercepto que minimizan la diferencia entre las predicciones y los valores reales.

</details>

:::

#### Evaluar desempeño del modelo

Una vez entrenado el modelo, evaluo su desempeño usando métricas estadísticas. Estas nos permiten cuantificar qué tan bien el modelo predice los valores de sólidos suspendidos a partir de la reflectancia en los datos de prueba.

```{python}
y_pred = regressor.predict(X_test)
p_rmse = mean_squared_error(y_test, y_pred)
p_r2 = r2_score(y_test, y_pred)

```

::: {.callout-note title="Métricas de desempeño"}

```{python}
#| echo: false

print("El error cuadrático medio es:", round(p_rmse, 3))
print("El coeficiente de determinación (R²) es:", round(p_r2, 3))
```
:::

::: {.dropdown}
<details>
<summary>📄 Nota técnica</summary>

> `predict()` genera predicciones del modelo usando los datos de prueba. Estas predicciones se comparan con los valores reales para evaluar el desempeño.

> `mean_squared_error` calcula el promedio de los errores al cuadrado. Cuanto menor sea este valor, mejor se ajusta el modelo.

> `r2_score` mide qué proporción de la variabilidad en los datos es explicada por el modelo. Un valor cercano a 1 indica una buena predicción.

</details>

:::

#### Visualizar el modelo

Finalmente, se visualiza gráficamente la relación entre reflectancia y sólidos suspendidos, tanto en el conjunto de entrenamiento como en el de prueba. Esto ayuda a interpretar de forma visual cómo se ajusta el modelo a los datos reales.

```{python}
fig, ax = plt.subplots(ncols=2, figsize=(10, 5), sharex=True, sharey=True)

# Gráfico entrenamiento
ax[0].plot(X_train, regressor.predict(X_train), linewidth=3, color="#17A77E", label="Modelo")
ax[0].scatter(X_train, y_train, label="Entrenamiento", color="#9D50A6", alpha=0.6)
ax[0].set(xlabel="Reflectancia", ylabel="Sol_Sus", title="Conjunto de entrenamiento")
ax[0].legend()

# Gráfico validación
ax[1].plot(X_test, y_pred, linewidth=3, color="#17A77E", label="Modelo")
ax[1].scatter(X_test, y_test, label="Validación", color="#9D50A6", alpha=0.6)
ax[1].set(xlabel="Reflectancia", ylabel="Sol_Sus", title="Conjunto de validación")
ax[1].legend()

fig.suptitle("Regresión lineal")

plt.show()
```

::: {.dropdown}
<details>
<summary>📄 Nota técnica</summary>

> `plt.subplots` crea una figura con uno o más ejes para dibujar. Permite organizar varios gráficos en una misma figura.

> `plot()` traza una línea continua. Se usa para mostrar la línea de regresión generada por el modelo.

> `scatter()` traza puntos individuales. Se usa para mostrar los datos reales y compararlos con la línea del modelo.

> `set()` configura etiquetas de ejes y títulos de los subgráficos.

> `legend()` muestra una leyenda que identifica cada elemento del gráfico.

> `fig.suptitle()` agrega un título general a la figura completa.

> `plt.show()` es necesario para visualizar los gráficos al renderizar el documento.

</details>

:::



### Regresión lineal simple usando una sola banda espectral (B01)

En este análisis voy a construir un modelo de regresión lineal simple para predecir la variable **sol_sus** a partir de una única variable independiente: la **banda espectral B01**. 

El objetivo es evaluar la capacidad de esta banda individual para explicar el comportamiento de la variable de interés.

::: {.callout-tip title="¿Qué cambió respecto al ejemplo anterior?"}
En el ejemplo anterior, el conjunto de datos contiene reflectancia medida en distintas bandas espectrales, identificadas en la columna `banda` (como "B01", "B02", etc.).
Ahora, para el análisis, selecciono solo las filas donde `banda` es `B01`, usando sus valores de reflectancia (`reflect`) como variable independiente `X` para predecir los sólidos suspendidos (`sol_sus`).

:::

#### Importación de librerías

```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
```

#### Carga de datos

```{python}
df = pd.read_csv('datos/datos_sol_sus_pixel_3x3.csv')
df.head()
```

#### Selección de variables y división de datos

```{python}
df_B01 = df[df['banda'] == 'B01']

X = df_B01[['reflect']]  # Variable independiente: reflectancia para B01
y = df_B01['sol_sus']    # Variable dependiente

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, shuffle=False
)
```

::: {.dropdown}
<details>
<summary>📄 Nota técnica</summary>

> `df[df['banda'] == 'B01']` es un filtro en pandas para seleccionar solo las filas donde la columna "banda" tenga el valor "B01". El resultado es un nuevo DataFrame con solo esas filas.

</details>

:::

#### Entrenamiento del modelo

```{python}
regressor = LinearRegression().fit(X_train, y_train)
```

#### Evaluación del modelo

```{python}
y_pred = regressor.predict(X_test)
p_rmse = mean_squared_error(y_test, y_pred)
p_r2 = r2_score(y_test, y_pred)
```

::: {.callout-note title="Métricas de desempeño"}

```{python}
#| echo: false
print(f"Error cuadrático medio (RMSE): {p_rmse:.3f}")
print(f"Coeficiente de determinación (R²): {p_r2:.3f}")
```
:::

#### Visualización de resultados

```{python}
fig, ax = plt.subplots(ncols=2, figsize=(10, 5), sharex=True, sharey=True)

# Conjunto de entrenamiento
ax[0].plot(X_train, regressor.predict(X_train), linewidth=3, color="#17A77E", label="Modelo")
ax[0].scatter(X_train, y_train, label="Entrenamiento", color="#9D50A6", alpha=0.6)
ax[0].set(xlabel="B01", ylabel="Sol_sus", title="Conjunto de entrenamiento")
ax[0].legend()

# Conjunto de validación
ax[1].plot(X_test, y_pred, linewidth=3, color="#17A77E", label="Modelo")
ax[1].scatter(X_test, y_test, label="Validación", color="#9D50A6", alpha=0.6)
ax[1].set(xlabel="B01", ylabel="Sol_sus", title="Conjunto de validación")
ax[1].legend()

fig.suptitle("Regresión lineal - B01 vs Sol_sus")
plt.show()
```