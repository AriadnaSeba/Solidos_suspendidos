---
title: "Análisis de Sólidos Suspendidos"
format: 
  html:
    number-sections: true
    toc: true
    embed-resources: true
    crossrefs-hover: false
    lang: es
    bibliography: bibliografia/bibliografia.bib
    csl: bibliografia/ieee.csl
date: last-modified
author:
  - name: Víctor Gauto
    orcid: 0000-0001-9960-8558
    corresponding: true
    email: victor.gauto@ca.frre.utn.edu.ar
    affiliations:
      - name: GISTAQ (UTN-FRRe)
        url: https://www.instagram.com/gistaq.utn/
abstract: |
  Este sitio web contiene cuestiones etc
keywords:
  - GISTAQ
  - UTN
  - FRRe
  - Quarto
---

## Sólidos suspendidos totales<span style="font-weight:normal; font-size: 1rem">, por Vera Geneyer (https://github.com/VeraGeneyer)</span> {toc-text="Sólidos suspendidos totales"}

Los sólidos suspendidos totales (TSM): es la cantidad de materia en suspensión en el agua, que incluye plancton, minerales, arena, y microorganismos. Se determinan como el residuo no filtrable de una muestra de agua. Niveles altos (TSM) pueden reducir la transparencia del agua, limitar la luz y y transportar sustancias tóxicas, afectando la vida acuática y la calidad del agua.
Este parámetro, medido mediante sensores remotos, nos da información sobre el estado físico del cuerpo de agua y están relacionados con factores como la humedad, temperatura y entre otros, que es vital para detectar riesgos al ecosistema y cumplir con las normas ambientales.

### Métodos tradicionales

:::: {.content-visible when-format="html"}

::: {.column-screen-right}

| Ecuación | Bandas (nm) | Métricas | Aguas | Plataforma | Referencia |
|:-:|:--|:--|:--|:--|:-:|
| $-229.34 \left( \frac{B03}{B08} \right)^{3}+1001.65 \left( \frac{B03}{B08} \right)^{2}-1422.7 \left( \frac{B03}{B08} \right)+665.17$ | B03, B08 | $R^{2}$ | Embalse^[Aguas lénticas.] | Landsat-8 | @Ramirez2017 |
| $-244.83+40.21 \cdot B01-3.67 \cdot NDWI$ | B01, NDWI (B03, B08) | $R^{2}$, RMSE, d | Río^[d = prueba estadística de <b>Durbin-Watson</b>.] | GeoEye | @Gomez2014 |

: Características principales de algoritmos tradicionales para la estimación de sólidos suspendidos. {#tbl-solsus-trad .striped .hover tbl-colwidths="[50,10,10,10,10]"}

:::

::::

:::: {.content-visible when-format="typst"}

| Ecuación | Referencia |
|:--|:-:|
| $-229.34 \left( \frac{B03}{B08} \right)^{3}+1001.65 \left( \frac{B03}{B08} \right)^{2}-1422.7 \left( \frac{B03}{B08} \right)+665.17$ | [@Ramirez2017] |
| $-244.83+40.21 \cdot B01-3.67 \cdot NDWI$ | [@Gomez2014] |

: Características principales de algoritmos tradicionales para la estimación de sólidos suspendidos. {#tbl-solsus-trad .striped .hover tbl-colwidths="[80,20]"}

::: {.block stroke='rgb("#B86092")' inset="8pt" radius="4pt"}

[Ver tabla completa en la versión online &#x2197;](https://vhgauto.quarto.pub/gistaq-parana/#tbl-solsus-trad)

:::

::::

De acuerdo a un estudio que analizó 48 cuerpos de agua, la estimación de TSM se hizo en su mayoría por modelos lineales, siendo la banda B8A la más frecuente [@Cruz2023].

### Métodos de aprendizaje automático

El **aprendizaje automático (ML)**  es una rama de la inteligencia artificial cuyo objetivo es desarrollar algoritmos capaces de resolver problemas mediante el análisis de datos y la creación de funciones que describen el comportamiento de fenómenos monitoreados [@Carpio2021]. Los modelos de aprendizaje automático más utilizados y mencionados por los investigadores para predecir la concentración de SST son:

* **Bosque Aleatorio (RF) y Refuerzo Adaptativo (AdB)**, modelos que se destacan por su robustez ante datos complejos y ruidosos. Estos algoritmos construyen múltiples árboles de decisión que analizan las relaciones entre características como el uso del suelo o el volumen de escorrentía y los niveles de SST [@Moeini2021].

* **Redes Neuronales Artificiales (ANN)**, copian las redes neuronales biológicas y aprenden patrones complejos en grandes volúmenes de datos, como los niveles de SST en distintas condiciones ambientales [@Moeini2021],

* **k-Nearest Neighbors (kNN)**, en sus variantes de ponderación uniforme y variable, que estima el SST en función de la cercanía en características de nuevos puntos de muestreo con datos históricos [@Moeini2021].

El aprendizaje automático es esencial para mejorar la precisión y rapidez en el análisis de la calidad del agua, proporcionando un monitoreo más eficiente y menos costoso en comparación con los métodos tradicionales, especialmente en áreas de difícil acceso o con datos limitados.

:::: {.content-visible when-format="html"}

::: {.column-screen-right}

| **Modelo de machine learning** | **Software** | **Agua** | **Datos** | **Métricas** | **Referencias** |
|:--|:--|:--|:--|:--|:-:|
|Bagging y Random Forest|Programa R|Bahía|Muestreo|Prueba de normalidad multivalente Mardia-tests y Royston|@Carpio2021|
|Regresión lineal, LASSO, regresión de vectores de soporte (SVR), K vecinos más cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).|-|Lago y embalse|Sentinel-2 y UAV|$R^{2}$| @Silveira2020|
|Regresión lineal, regresión de vectores de soporte (SVR), K vecinos más cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).|Programa Python|Lagos|Estación de monitoreo (Sensores para cada parámetro)|$R^{2}$, NSE y RMSE| @Moeini2021|

: Características principales de algoritmos de aprendizaje automático para la estimación de sólidos suspendidos. {#tbl-solsus-machine .striped .hover tbl-colwidths="[40,12,12,13,13,10]"}

:::

::::

:::: {.content-visible when-format="typst"}

| Modelo de machine learning | Referencias |
|:--|:-:|
|Bagging y Random Forest| [@Carpio2021] |
|Regresión lineal, LASSO, regresión de vectores de soporte (SVR), K vecinos más cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).| [@Silveira2020] |
|Regresión lineal, regresión de vectores de soporte (SVR), K vecinos más cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).| [@Moeini2021] |

: Características principales de algoritmos de aprendizaje automático para la estimación de sólidos suspendidos. {#tbl-solsus-machine .striped .hover tbl-colwidths="[80,20]"}

::: {.block stroke='rgb("#B86092")' inset="8pt" radius="4pt"}

[Ver tabla completa en la versión online &#x2197;](https://vhgauto.quarto.pub/gistaq-parana/#tbl-solsus-machine)

:::

::::

Diferentes metodologías para la resolución de una regresión simple por el método de mínimos cuadrados.

## **Python**, con `sklearn`

Se muestra a continuación el tutorial mostrado en para el modelo [Mínimos cuadrados ordinarios](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#ordinary-least-squares-example).

Importo la librería `sklearn`, funciones de interés y para generar figuras.

```{python}
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
```

Cargo los datos de interés y divido en entrenamiento y validación.

```{python}
X, y = load_diabetes(return_X_y=True)
X = X[:, [2]]  # Use only one feature
X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=20, shuffle=False
)
```

Creo un modelo de regresión y ajusto utilizando los datos de entrenamiento.

```{python}
regressor = LinearRegression().fit(X_train, y_train)
```

Evalúo el modelo generado a partir de las **métricas de desempeño**.

```{python}
y_pred = regressor.predict(X_test)
p_rmse = mean_squared_error(y_test, y_pred)
p_r2 = r2_score(y_test, y_pred)
```

::: {.callout-note icon="false" title="Métricas de desempeño"}
El error cuadrático medio es: `{r} round(py$p_rmse, 3)`.

El coeficiente de determinación (R<sup>2</sup>) es: `{r} round(py$p_r2, 3)`.
:::

Visualizo los resultados comparando el conjunto de entrenamiento y validación.

::: {.column-body-outset}

```{python}
fig, ax = plt.subplots(ncols=2, figsize=(10, 5), sharex=True, sharey=True)

ax[0].plot(
    X_train,
    regressor.predict(X_train),
    linewidth=3,
    color="#17A77E",
    label="Modelo",
)
ax[0].scatter(X_train, y_train, label="Entrenamiento", color = "#9D50A6", alpha = .6)
ax[0].set(xlabel="Característica", ylabel="Objetivo", title="Conjunto de entrenamiento")
ax[0].legend()

ax[1].plot(X_test, y_pred, linewidth=3, color="#17A77E", label="Modelo")
ax[1].scatter(X_test, y_test, label="Validación", color = "#9D50A6", alpha = .6)
ax[1].set(xlabel="Característica", ylabel="Objetivo", title="Conjunto de validación")
ax[1].legend()

fig.suptitle("Regresión lineal")

plt.show()
```


### Procesamiento de datos ambientales con Python

Se detalla el procedimiento técnico que implementé para procesar información ambiental georreferenciada con el objetivo de analizar el comportamiento del parámetro **sólidos suspendidos (sol_sus)** en una región específica (pixel `3x3`). Para esto, utilicé el lenguaje Python y la biblioteca `pandas`, que resulta particularmente eficiente para el manejo de estructuras tabulares. 

#### Carga de datos

Primero importo la biblioteca `pandas`, una herramienta en Python que se utiliza para manejar datos en formato tabular (como hojas de cálculo o CSVs). Le damos el alias `pd` por convención, para simplificar el código.

Luego cargo dos archivos CSV con la función `pd.read_csv()`, la cual convierte dichos archivos en objetos del tipo `DataFrame`, que representan tablas en memoria, que son estructuras de datos similares a tablas (parecida a una hoja de Excel). Los conjuntos de datos cargados fueron:

- `gis_df`: contiene información geográfica (latitud, longitud, pixel, etc.).
- `lab_df`: contiene datos de laboratorio, incluyendo el parámetro de interés `sol_sus`.

 Verifico la carga correcta mostrando las primeras filas con la función `.head()`. Es útil para ver rápidamente cómo es la estructura del archivo: qué columnas hay, qué tipo de datos, si se cargó bien.

```{python}
import pandas as pd  # pandas es la biblioteca para manejar datos tabulares

# Cargar los archivos de datos
gis_df = pd.read_csv('datos/base_de_datos_gis.csv')
lab_df = pd.read_csv('datos/base_de_datos_lab.csv')

# Ver las primeras filas para asegurarse de que se cargaron bien
gis_df.head(), lab_df.head()
```

::: {.dropdown}
<details>
<summary>Nota técnica</summary>

- `pd.read_csv()` carga los archivos en estructuras llamadas *dataframes*, que funcionan como tablas.  
- `head()` te muestra las primeras 5 filas para ver cómo están los datos.

</details>

:::

#### Filtrar el parámetro 'sol_sus'

En el conjunto de datos del laboratorio `lab_df`, hay múltiples parámetros medidos (como pH, turbidez, etc.). En este caso, me interesa trabajar solamente con los datos de **sólidos suspendidos**, identificado como `"sol_sus"` en la columna `param`. Este filtrado selectivo lo realicé para limitar el análisis al fenómeno físico-químico de interés.

Filtré el DataFrame para quedarme solo con esas filas, y renombré la columna `valor` como `sol_sus` para que sea más claro en los siguientes pasos.

```{python}
# Filtramos solo las filas donde el parámetro es 'sol_sus'
sol_sus_df = lab_df[lab_df["param"] == "sol_sus"]

# Renombramos la columna 'valor' a 'sol_sus' para que tenga sentido en el merge
sol_sus_df = sol_sus_df.rename(columns={"valor": "sol_sus"})

# Mostramos para confirmar
sol_sus_df.head()
```

::: {.dropdown}
<details>
<summary>Nota técnica</summary>

- `lab_df[lab_df["param"] == "sol_sus"]` filtra las filas cuyo valor en la columna `"param"` sea `"sol_sus"`.  
- `.rename(columns={"valor": "sol_sus"})` cambia el nombre de la columna `"valor"` a `"sol_sus"`.

</details>

:::

#### Combinar datos geoespaciales y de laboratorio

Ahora quiero unir los datos de ubicación (latitud, longitud) con los valores de sol_sus. Para eso uní ambos conjuntos de datos usando `pd.merge()`, que combina tablas usando columnas comunes.
En este caso, usé las columnas `latitud` y `longitud` para juntar las filas que corresponden a la misma ubicación.
La unión fue de tipo `"inner"`, que mantiene solo las filas que aparecen en ambas tablas, asegurando que solo trabajemos con datos coincidentes.

```{python}
# Unimos por latitud y longitud
df_combinado = pd.merge(
    gis_df,
    sol_sus_df[["latitud", "longitud", "sol_sus"]],
    on=["latitud", "longitud"],
    how="inner"
)

df_combinado.head()
```

::: {.dropdown}
<details>
<summary>Nota técnica</summary>

- `pd.merge()` permite combinar dos DataFrames en uno nuevo, uniendo filas que coincidan en las columnas especificadas.  
- `on=["latitud", "longitud"]` indica que la combinación debe hacerse usando esas columnas como claves.  
- `how="inner"` especifica el tipo de combinación:  
  - `"inner"`: solo conserva las filas donde hay coincidencia en ambos DataFrames.  
  - Otras opciones:  
    - `"left"`: conserva todas las filas del primer DataFrame.  
    - `"right"`: conserva todas las filas del segundo.  
    - `"outer"`: conserva todo, incluso si no hay coincidencia.

</details>

:::

#### Filtrado espacial por pixel

Luego de combinar los datos, aplico un filtrado adicional al DataFrame sobre la columna `pixel` para conservar únicamente las filas correspondientes al área geográfica designada como `"3x3"`. Este paso reduce el dominio de análisis y permite concentrarse en una región de estudio concreta.

```{python}
# Filtramos solo los datos del pixel 3x3
df_pixel_3x3 = df_combinado[df_combinado["pixel"] == "3x3"]

df_pixel_3x3.head()
```

::: {.dropdown}
<details>
<summary>Nota técnica</summary>

- Usa filtrado booleano (`DataFrame[condición]`), que es la forma estándar en pandas para seleccionar subconjuntos de datos. 
- `df_pixel_3x3 = df_combinado[df_combinado["pixel"] == "3x3"]` selecciona ese subconjunto. Filtra las filas cuyo valor en la columna `"pixel"` es igual a `"3x3"`.

</details>

:::

#### Reordenamiento de columnas

Para facilitar la lectura y el análisis del `DataFrame` final, reorganicé las columnas de manera que las variables `fecha`, `longitud` y `latitud` figuren al comienzo. 

```{python}
# Reordenar las columnas
columnas_ordenadas = ['fecha', 'longitud', 'latitud'] + [col for col in df_pixel_3x3.columns if col not in ['fecha', 'longitud', 'latitud']]
df_reordenado = df_pixel_3x3[columnas_ordenadas]

# Mostramos las primeras filas para verificar
df_reordenado.head()
```

::: {.dropdown}
<details>
<summary>Nota técnica</summary>

- `[col for col in df_pixel_3x3.columns if col not in [...]]` genera una lista con todas las columnas excepto las que se indican explícitamente. Esto permite personalizar el orden de las columnas de forma flexible.   
- `['fecha', 'longitud', 'latitud'] + [...]` reordena las columnas.  
- `df_pixel_3x3[columnas_ordenadas]` reordena efectivamente las columnas del DataFrame con base en la lista definida.

</details>

:::

#### Guardar el archivo final

Finalmente, guardo el resultado como un nuevo archivo .csv dentro de la carpeta datos/. 

Por último, exporté el resultado a un nuevo archivo en formato `.csv`, mediante la función `to_csv()` de pandas, con el parámetro `index=False` para evitar que la columna de índice se incluya en el archivo de salida que pandas crea por defecto.
Esto me permite utilizarlo después para visualización o análisis posterior.

```{python}
# Guardar el archivo CSV dentro de la carpeta "datos"
df_reordenado.to_csv('datos/datos_sol_sus_pixel_3x3.csv', index=False)
```

::: {.dropdown}
<details>
<summary>Nota técnica</summary>

- `to_csv()`  guarda los datos en formato CSV.  
- `index=False` evita que se guarde el índice numérico del DataFrame como una columna adicional en el CSV.

</details>

:::


